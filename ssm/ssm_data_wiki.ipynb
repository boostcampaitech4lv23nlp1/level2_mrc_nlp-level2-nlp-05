{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80482583-b9b5-4d9c-b646-46f5af80567a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 08:16:39 | INFO | pororo.models.brainbert.tasks.sequence_tagging | [input] dictionary: 4005 types\n",
      "2023-01-05 08:16:39 | INFO | pororo.models.brainbert.tasks.sequence_tagging | [label] dictionary: 41 types\n"
     ]
    }
   ],
   "source": [
    "from pororo import Pororo\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    ")\n",
    "\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "mu, sigma = 0, 0.1\n",
    "ner = Pororo(task=\"ner\", lang=\"ko\")\n",
    "\n",
    "model_name = 'klue/roberta-large'\n",
    "\n",
    "# set up tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc4b14e8-ab6e-49a6-bb83-18a7aba070d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data():\n",
    "    dataset_path = \"../dataset/wikipedia_documents.json\"\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        wiki = json.load(f)\n",
    "\n",
    "    wiki_texts_prev = list(dict.fromkeys([v[\"text\"] for v in wiki.values()]))\n",
    "    \n",
    "    wiki_texts = []\n",
    "    for t in wiki_texts_prev:\n",
    "        while(len(t) > 500):\n",
    "            wiki_texts.append(t[:500])\n",
    "            t = t[384:]  # stride = 500 - 384 \n",
    "    \n",
    "    wiki_strings = []\n",
    "    wiki_labels = []\n",
    "    wiki_inputs = []\n",
    "    for text in tqdm(wiki_texts):\n",
    "        string = ''\n",
    "        labels = []\n",
    "        inputs = []\n",
    "        for pred in ner(text):\n",
    "            if pred[1] != 'O' and np.random.normal(mu, sigma) > 0:\n",
    "                mask = tokenizer(pred[0])['input_ids'][1:-1]\n",
    "                ln = len(mask)\n",
    "                labels += mask\n",
    "                inputs += tokenizer('[MASK]' * ln)['input_ids'][1:-1]\n",
    "                string += '[MASK]' * ln\n",
    "\n",
    "            else:\n",
    "                labels += tokenizer(pred[0])['input_ids'][1:-1]\n",
    "                inputs += tokenizer(pred[0])['input_ids'][1:-1]\n",
    "                string += pred[0]\n",
    "        wiki_labels.append(labels)\n",
    "        wiki_inputs.append(inputs)\n",
    "        wiki_strings.append(string)\n",
    "            \n",
    "    wiki_corpus = [{\"document_text\": wiki_texts[i], \"masked_strings\": wiki_strings[i], \"inputs\":wiki_inputs[i], \"labels\":wiki_labels[i]} for i in range(len(wiki_texts))]\n",
    "    return wiki_corpus\n",
    "\n",
    "def make_data():\n",
    "    with open(\"../dataset/wikipedia_documents_ssm_qa.json\", \"w\") as f:\n",
    "        json.dump(load_data(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcaa30f9-04fd-42a6-a6d6-59589b62fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89ac06f5-e6ae-43c7-9fa2-57d37d7edc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63720\n",
      "{'document_text': '이 목록에 실린 국가 기준은 1933년 몬테비데오 협약 1장을 참고로 하였다. 협정에 따르면, 국가는 다음의 조건을 만족해야 한다.\\n* (a) 영속적인 국민\\n* (b) 일정한 영토\\n* (c) 정부\\n* (d) 타국과의 관계 참여 자격.\\n특히, 마지막 조건은 국제 공동체의 참여 용인을 내포하고 있기 때문에, 다른 나라의 승인이 매우 중요한 역할을 할 수 있다.  이 목록에 포함된 모든 국가는 보통 이 기준을 만족하는 것으로 보이는 자주적이고 독립적인 국가이다. 하지만 몬테비데오 협약 기준을 만족하는지의 여부는 많은 국가가 논쟁이 되고 있는 실정이다. 또한, 몬테비데오 협약 기준만이 국가 지위의 충분한 자격이든 아니든, 국제법의 견해 차이는 존재할 수 있다. 이 물음에 대한 다른 이론에 대한 고리는 아래에서 볼 수 있다.\\n\\n위 기준에 논거하여 이 목록은 다음 206개 국가를 포함하고 있다.\\n* 일반 국제 승인을 받은 195개 자주 국가.\\n** 유엔 가입 국가 193개\\n** 성좌의 명칭으로', 'masked_strings': '이 목록에 실린 국가 기준은 1933년 몬테비데오 협약 [MASK][MASK]을 참고로 하였다. 협정에 따르면, 국가는 다음의 조건을 만족해야 한다. * ([MASK]) 영속적인 국민 * [MASK][MASK]) 일정한 영토 * (c) [MASK] * (d) 타국과의 관계 참여 자격. 특히, 마지막 조건은 국제 공동체의 참여 용인을 내포하고 있기 때문에, 다른 나라의 승인이 매우 중요한 역할을 할 수 있다. 이 목록에 포함된 모든 국가는 보통 이 기준을 만족하는 것으로 보이는 자주적이고 독립적인 국가이다. 하지만 [MASK][MASK][MASK]오 협약 기준을 만족하는지의 여부는 많은 국가가 논쟁이 되고 있는 실정이다. 또한, 몬테비데오 협약 기준만이 국가 지위의 충분한 자격이든 아니든, [MASK]의 견해 차이는 존재할 수 있다. 이 물음에 대한 다른 이론에 대한 고리는 아래에서 볼 수 있다. 위 기준에 논거하여 이 목록은 다음 [MASK][MASK] 국가를 포함하고 있다. * 일반 국제 승인을 받은 195개 자주 국가. ** 유엔 가입 국가 193개 ** 성좌의 명칭으로', 'inputs': [1504, 10188, 2170, 11381, 3728, 3872, 2073, 20998, 2440, 25218, 2151, 28330, 5598, 4, 4, 1498, 6108, 2200, 1889, 2507, 2062, 18, 6323, 2170, 3881, 2460, 16, 3728, 2259, 3729, 2079, 4423, 2069, 4671, 8084, 3605, 18, 14, 12, 4, 13, 27515, 31221, 3735, 14, 4, 4, 13, 4362, 2470, 8683, 14, 12, 70, 13, 4, 14, 12, 71, 13, 24480, 2145, 2079, 3654, 3844, 5109, 18, 3727, 16, 4178, 4423, 2073, 3854, 5516, 2079, 3844, 6623, 2069, 11160, 19521, 1513, 2015, 3624, 2170, 16, 3656, 3779, 2079, 5887, 2052, 4230, 3748, 2470, 4008, 2069, 1892, 1295, 1513, 2062, 18, 1504, 10188, 2170, 3954, 2897, 3721, 3728, 2259, 4865, 1504, 3872, 2069, 4671, 2205, 2259, 575, 6233, 3783, 2259, 4695, 2125, 2052, 2088, 4751, 31221, 3728, 28674, 18, 3696, 4, 4, 4, 1443, 5598, 3872, 2069, 4671, 2205, 18246, 2079, 4588, 2259, 1039, 2073, 3728, 2116, 6851, 2052, 859, 2088, 1513, 2259, 7123, 28674, 18, 3819, 16, 25218, 2151, 28330, 5598, 3872, 2154, 2052, 3728, 6401, 2079, 4332, 2470, 5109, 2052, 2778, 3614, 2778, 16, 4, 1503, 6270, 4193, 2259, 3990, 2085, 1295, 1513, 2062, 18, 1504, 9164, 2170, 3618, 3656, 4748, 2170, 3618, 7403, 2259, 4402, 3604, 1164, 1295, 1513, 2062, 18, 1485, 3872, 2170, 25307, 7488, 1504, 10188, 2073, 3729, 4, 4, 3728, 2138, 3954, 19521, 1513, 2062, 18, 14, 3935, 3854, 5887, 2069, 1122, 2073, 6012, 2019, 4695, 3728, 18, 14, 14, 6125, 4502, 3728, 6026, 2019, 14, 14, 1268, 2584, 2079, 7864, 6233], 'labels': [1504, 10188, 2170, 11381, 3728, 3872, 2073, 20998, 2440, 25218, 2151, 28330, 5598, 21, 2121, 1498, 6108, 2200, 1889, 2507, 2062, 18, 6323, 2170, 3881, 2460, 16, 3728, 2259, 3729, 2079, 4423, 2069, 4671, 8084, 3605, 18, 14, 12, 68, 13, 27515, 31221, 3735, 14, 12, 69, 13, 4362, 2470, 8683, 14, 12, 70, 13, 3659, 14, 12, 71, 13, 24480, 2145, 2079, 3654, 3844, 5109, 18, 3727, 16, 4178, 4423, 2073, 3854, 5516, 2079, 3844, 6623, 2069, 11160, 19521, 1513, 2015, 3624, 2170, 16, 3656, 3779, 2079, 5887, 2052, 4230, 3748, 2470, 4008, 2069, 1892, 1295, 1513, 2062, 18, 1504, 10188, 2170, 3954, 2897, 3721, 3728, 2259, 4865, 1504, 3872, 2069, 4671, 2205, 2259, 575, 6233, 3783, 2259, 4695, 2125, 2052, 2088, 4751, 31221, 3728, 28674, 18, 3696, 25218, 2151, 2147, 1443, 5598, 3872, 2069, 4671, 2205, 18246, 2079, 4588, 2259, 1039, 2073, 3728, 2116, 6851, 2052, 859, 2088, 1513, 2259, 7123, 28674, 18, 3819, 16, 25218, 2151, 28330, 5598, 3872, 2154, 2052, 3728, 6401, 2079, 4332, 2470, 5109, 2052, 2778, 3614, 2778, 16, 20747, 1503, 6270, 4193, 2259, 3990, 2085, 1295, 1513, 2062, 18, 1504, 9164, 2170, 3618, 3656, 4748, 2170, 3618, 7403, 2259, 4402, 3604, 1164, 1295, 1513, 2062, 18, 1485, 3872, 2170, 25307, 7488, 1504, 10188, 2073, 3729, 21732, 2019, 3728, 2138, 3954, 19521, 1513, 2062, 18, 14, 3935, 3854, 5887, 2069, 1122, 2073, 6012, 2019, 4695, 3728, 18, 14, 14, 6125, 4502, 3728, 6026, 2019, 14, 14, 1268, 2584, 2079, 7864, 6233]}\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"../dataset/wikipedia_documents_ssm2.json\"\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    wiki = json.load(f)\n",
    "    print((len(wiki)))\n",
    "    \n",
    "for i in range(1):\n",
    "    print(wiki[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9937157e-d659-427e-9275-9df0758639e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 09:53:04 | WARNING | datasets.builder | Using custom data configuration default-e96911525636eee5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /opt/ml/.cache/huggingface/datasets/json/default-e96911525636eee5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 5384.22it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 581.33it/s]\n",
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /opt/ml/.cache/huggingface/datasets/json/default-e96911525636eee5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 198.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document_text', 'masked_strings', 'inputs', 'labels'],\n",
       "        num_rows: 63720\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files='../dataset/wikipedia_documents_ssm.json')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26bc250-dcb8-4622-bf27-2bd4ea082932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data2():\n",
    "    dataset_path = \"./dataset/wikipedia_documents.json\"\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        wiki = json.load(f)\n",
    "\n",
    "    wiki_texts_prev = list(dict.fromkeys([v[\"text\"] for v in wiki.values()]))\n",
    "    \n",
    "    wiki_texts = []\n",
    "    for t in wiki_texts_prev:\n",
    "        while(len(t) > 500):\n",
    "            wiki_texts.append(t[:500])\n",
    "            t = t[384:]  # stride = 500 - 384 \n",
    "    \n",
    "    wiki_strings = []\n",
    "    wiki_labels = []\n",
    "    wiki_inputs = []\n",
    "    for text in tqdm(wiki_texts):\n",
    "        string = ''\n",
    "        labels = []\n",
    "        inputs = []\n",
    "        for pred in ner(text):\n",
    "            if pred[1] != 'O' and np.random.normal(mu, sigma) < 0:\n",
    "                mask = tokenizer(pred[0])['input_ids'][1:-1]\n",
    "                ln = len(mask)\n",
    "                labels += mask\n",
    "                inputs += tokenizer('[MASK]' * ln)['input_ids'][1:-1]\n",
    "                string += '[MASK]' * ln\n",
    "\n",
    "            else:\n",
    "                labels += tokenizer(pred[0])['input_ids'][1:-1]\n",
    "                inputs += tokenizer(pred[0])['input_ids'][1:-1]\n",
    "                string += pred[0]\n",
    "        wiki_labels.append(labels)\n",
    "        wiki_inputs.append(inputs)\n",
    "        wiki_strings.append(string)\n",
    "            \n",
    "    wiki_corpus = [{\"document_text\": wiki_texts[i], \"masked_strings\": wiki_strings[i], \"inputs\":wiki_inputs[i], \"labels\":wiki_labels[i]} for i in range(len(wiki_texts))]\n",
    "    return wiki_corpus\n",
    "\n",
    "def make_data():\n",
    "    with open(\"./dataset/wikipedia_documents_ssm_qa2.json\", \"w\") as f:\n",
    "        json.dump(load_data(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5e89e-f291-4514-bc23-ccbc216357e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa7f7c-7d99-45d6-af9d-e98e2ae52b83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
