{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350c16ce-8f43-4253-b78a-d599221ee1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63720\n",
      "63720\n",
      "127440\n",
      "{'document_text': '이 목록에 실린 국가 기준은 1933년 몬테비데오 협약 1장을 참고로 하였다. 협정에 따르면, 국가는 다음의 조건을 만족해야 한다.\\n* (a) 영속적인 국민\\n* (b) 일정한 영토\\n* (c) 정부\\n* (d) 타국과의 관계 참여 자격.\\n특히, 마지막 조건은 국제 공동체의 참여 용인을 내포하고 있기 때문에, 다른 나라의 승인이 매우 중요한 역할을 할 수 있다.  이 목록에 포함된 모든 국가는 보통 이 기준을 만족하는 것으로 보이는 자주적이고 독립적인 국가이다. 하지만 몬테비데오 협약 기준을 만족하는지의 여부는 많은 국가가 논쟁이 되고 있는 실정이다. 또한, 몬테비데오 협약 기준만이 국가 지위의 충분한 자격이든 아니든, 국제법의 견해 차이는 존재할 수 있다. 이 물음에 대한 다른 이론에 대한 고리는 아래에서 볼 수 있다.\\n\\n위 기준에 논거하여 이 목록은 다음 206개 국가를 포함하고 있다.\\n* 일반 국제 승인을 받은 195개 자주 국가.\\n** 유엔 가입 국가 193개\\n** 성좌의 명칭으로', 'masked_strings': '이 목록에 실린 국가 기준은 [MASK][MASK] [MASK][MASK][MASK][MASK] [MASK][MASK]을 참고로 하였다. 협정에 따르면, 국가는 다음의 조건을 만족해야 한다. * ([MASK]) 영속적인 [MASK] * (b) 일정한 영토 * (c) [MASK] * (d) 타국과의 관계 참여 자격. 특히, 마지막 조건은 국제 공동체의 참여 용인을 내포하고 있기 때문에, 다른 나라의 승인이 매우 중요한 역할을 할 수 있다. 이 목록에 포함된 모든 국가는 보통 이 기준을 만족하는 것으로 보이는 자주적이고 독립적인 국가이다. 하지만 [MASK][MASK][MASK]오 협약 기준을 만족하는지의 여부는 많은 국가가 논쟁이 되고 있는 실정이다. 또한, [MASK][MASK][MASK] 협약 기준만이 국가 지위의 충분한 자격이든 아니든, [MASK]의 견해 차이는 존재할 수 있다. 이 물음에 대한 다른 이론에 대한 고리는 [MASK]에서 볼 수 있다. 위 기준에 논거하여 이 목록은 다음 206개 국가를 포함하고 있다. * 일반 국제 승인을 받은 [MASK][MASK] 자주 국가. ** 유엔 가입 국가 193개 ** 성좌의 명칭으로', 'inputs': [1504, 10188, 2170, 11381, 3728, 3872, 2073, 4, 4, 4, 4, 4, 4, 4, 4, 1498, 6108, 2200, 1889, 2507, 2062, 18, 6323, 2170, 3881, 2460, 16, 3728, 2259, 3729, 2079, 4423, 2069, 4671, 8084, 3605, 18, 14, 12, 4, 13, 27515, 31221, 4, 14, 12, 69, 13, 4362, 2470, 8683, 14, 12, 70, 13, 4, 14, 12, 71, 13, 24480, 2145, 2079, 3654, 3844, 5109, 18, 3727, 16, 4178, 4423, 2073, 3854, 5516, 2079, 3844, 6623, 2069, 11160, 19521, 1513, 2015, 3624, 2170, 16, 3656, 3779, 2079, 5887, 2052, 4230, 3748, 2470, 4008, 2069, 1892, 1295, 1513, 2062, 18, 1504, 10188, 2170, 3954, 2897, 3721, 3728, 2259, 4865, 1504, 3872, 2069, 4671, 2205, 2259, 575, 6233, 3783, 2259, 4695, 2125, 2052, 2088, 4751, 31221, 3728, 28674, 18, 3696, 4, 4, 4, 1443, 5598, 3872, 2069, 4671, 2205, 18246, 2079, 4588, 2259, 1039, 2073, 3728, 2116, 6851, 2052, 859, 2088, 1513, 2259, 7123, 28674, 18, 3819, 16, 4, 4, 4, 5598, 3872, 2154, 2052, 3728, 6401, 2079, 4332, 2470, 5109, 2052, 2778, 3614, 2778, 16, 4, 1503, 6270, 4193, 2259, 3990, 2085, 1295, 1513, 2062, 18, 1504, 9164, 2170, 3618, 3656, 4748, 2170, 3618, 7403, 2259, 4, 3604, 1164, 1295, 1513, 2062, 18, 1485, 3872, 2170, 25307, 7488, 1504, 10188, 2073, 3729, 21732, 2019, 3728, 2138, 3954, 19521, 1513, 2062, 18, 14, 3935, 3854, 5887, 2069, 1122, 2073, 4, 4, 4695, 3728, 18, 14, 14, 6125, 4502, 3728, 6026, 2019, 14, 14, 1268, 2584, 2079, 7864, 6233], 'labels': [1504, 10188, 2170, 11381, 3728, 3872, 2073, 20998, 2440, 25218, 2151, 28330, 5598, 21, 2121, 1498, 6108, 2200, 1889, 2507, 2062, 18, 6323, 2170, 3881, 2460, 16, 3728, 2259, 3729, 2079, 4423, 2069, 4671, 8084, 3605, 18, 14, 12, 68, 13, 27515, 31221, 3735, 14, 12, 69, 13, 4362, 2470, 8683, 14, 12, 70, 13, 3659, 14, 12, 71, 13, 24480, 2145, 2079, 3654, 3844, 5109, 18, 3727, 16, 4178, 4423, 2073, 3854, 5516, 2079, 3844, 6623, 2069, 11160, 19521, 1513, 2015, 3624, 2170, 16, 3656, 3779, 2079, 5887, 2052, 4230, 3748, 2470, 4008, 2069, 1892, 1295, 1513, 2062, 18, 1504, 10188, 2170, 3954, 2897, 3721, 3728, 2259, 4865, 1504, 3872, 2069, 4671, 2205, 2259, 575, 6233, 3783, 2259, 4695, 2125, 2052, 2088, 4751, 31221, 3728, 28674, 18, 3696, 25218, 2151, 2147, 1443, 5598, 3872, 2069, 4671, 2205, 18246, 2079, 4588, 2259, 1039, 2073, 3728, 2116, 6851, 2052, 859, 2088, 1513, 2259, 7123, 28674, 18, 3819, 16, 25218, 2151, 28330, 5598, 3872, 2154, 2052, 3728, 6401, 2079, 4332, 2470, 5109, 2052, 2778, 3614, 2778, 16, 20747, 1503, 6270, 4193, 2259, 3990, 2085, 1295, 1513, 2062, 18, 1504, 9164, 2170, 3618, 3656, 4748, 2170, 3618, 7403, 2259, 4402, 3604, 1164, 1295, 1513, 2062, 18, 1485, 3872, 2170, 25307, 7488, 1504, 10188, 2073, 3729, 21732, 2019, 3728, 2138, 3954, 19521, 1513, 2062, 18, 14, 3935, 3854, 5887, 2069, 1122, 2073, 6012, 2019, 4695, 3728, 18, 14, 14, 6125, 4502, 3728, 6026, 2019, 14, 14, 1268, 2584, 2079, 7864, 6233]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./dataset/wikipedia_documents_ssm.json\", \"r\") as f1, open(\"./dataset/wikipedia_documents_ssm1.json\", \"r\") as f2:\n",
    "    wiki1 = json.load(f1)\n",
    "    print(len(wiki1))\n",
    "    wiki2 = json.load(f2)\n",
    "    print(len(wiki2))\n",
    "    wiki3 = wiki1 + wiki2\n",
    "    print(len(wiki3))\n",
    "    print(wiki3[0])\n",
    "    with open(\"./dataset/wikipedia_documents_ssm2.json\", \"w\") as f:\n",
    "        json.dump(wiki3, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6473efd-b523-48c5-9150-adab3fcf16ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "import argparse\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "from collections.abc import Mapping\n",
    "from dataclasses import dataclass\n",
    "from random import randint\n",
    "from collections import defaultdict\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "import torch\n",
    "\n",
    "class MyDataCollatorWithPadding(DataCollatorWithPadding):\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        max_len = 0\n",
    "        for i in features:\n",
    "            if len(i[\"input_ids\"]) > max_len:\n",
    "                max_len = len(i[\"input_ids\"])\n",
    "        \n",
    "        batch = defaultdict(list)\n",
    "        for item in features:\n",
    "            for k in item:\n",
    "                item[k] = torch.Tensor(item[k])\n",
    "                padding_len = max_len - item[k].size(0)\n",
    "                if k == \"input_ids\":\n",
    "                    item[k] = torch.cat((item[k], torch.tensor([self.tokenizer.pad_token_id] * padding_len)), dim=0)\n",
    "                else:\n",
    "                    item[k] = torch.cat((item[k], torch.tensor([0] * padding_len)), dim=0)\n",
    "                batch[k].append(item[k])\n",
    "\n",
    "        for k in batch:\n",
    "            batch[k] = torch.stack(batch[k], dim=0)\n",
    "            batch[k] = batch[k].to(torch.long)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "    \n",
    "def load_eval_dataset(tokenizer, datasets):\n",
    "    # preprocessing\n",
    "    def prepare_validation_features(examples):\n",
    "        # truncation과 padding(length가 짧을때만)을 통해 toknization을 진행하며, stride를 이용하여 overflow를 유지합니다.\n",
    "        # 각 example들은 이전의 context와 조금씩 겹치게됩니다.\n",
    "        \n",
    "        final_examples = {}\n",
    "        final_examples['input_ids'] = examples['inputs']\n",
    "        final_examples['labels'] = examples['labels']\n",
    "        \n",
    "        final_examples['attention_mask'] = []\n",
    "        for i in examples['inputs']:\n",
    "            final_examples['attention_mask'].append([1]*len(i))\n",
    "        \n",
    "        return final_examples\n",
    "\n",
    "    train_dataset = datasets[\"train\"]\n",
    "\n",
    "    # Validation Feature 생성\n",
    "    train_dataset = train_dataset.map(\n",
    "        prepare_validation_features,\n",
    "        batched = True,\n",
    "        remove_columns=['document_text', 'masked_strings', 'inputs', 'labels']\n",
    "    )\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "def ssm_pretrain():\n",
    "    model_name = 'klue/roberta-large'\n",
    "\n",
    "    # set up tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    \n",
    "    # label 없이 가져오기 위해서 load_predict_dataset 사용\n",
    "    ### Refactoring 필요! ###\n",
    "    dataset = load_dataset(\"json\", data_files=\"./dataset/wikipedia_documents_ssm2.json\")\n",
    "    train_dataset = load_eval_dataset(tokenizer, dataset)\n",
    "\n",
    "    # Pretrained model for MaskedLM training\n",
    "    model_config = AutoConfig.from_pretrained(model_name)  # 모델 가중치 불러오기\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name, config=model_config)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "\n",
    "    data_collator = MyDataCollatorWithPadding(tokenizer)\n",
    "\n",
    "    # cuda out-of-memory 발생하여 fp16 = True 로 변경\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./klue-roberta-pretrained\",\n",
    "        learning_rate=3e-05,\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        save_steps=4000,\n",
    "        save_total_limit=2,\n",
    "        save_strategy=\"steps\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=100,\n",
    "        fp16=True, # 16비트로 변환\n",
    "        fp16_opt_level=\"O1\",\n",
    "        resume_from_checkpoint=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(\"./klue-roberta-pretrained\")  # pretrained_model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8717d1b0-5c31-4cad-850b-e9cf75828992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = [1,1,1,1,1]\n",
    "torch.Tensor(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bfec974-87fe-4b0d-b9a1-f2ddd01bc271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4123cfdacdd623e5\n",
      "Found cached dataset json (/opt/ml/.cache/huggingface/datasets/json/default-4123cfdacdd623e5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "100%|██████████| 1/1 [00:00<00:00, 513.76it/s]\n",
      "Loading cached processed dataset at /opt/ml/.cache/huggingface/datasets/json/default-4123cfdacdd623e5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-d31b2cc0ae2b3444.arrow\n"
     ]
    }
   ],
   "source": [
    "model_name = 'klue/roberta-large'\n",
    "\n",
    "# set up tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# label 없이 가져오기 위해서 load_predict_dataset 사용\n",
    "### Refactoring 필요! ###\n",
    "dataset = load_dataset(\"json\", data_files=\"./dataset/wikipedia_documents_ssm_qa2.json\")\n",
    "train_dataset = load_eval_dataset(tokenizer, dataset)\n",
    "\n",
    "# Pretrained model for MaskedLM training\n",
    "model_config = AutoConfig.from_pretrained(model_name)  # 모델 가중치 불러오기\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = AutoModelForMaskedLM.from_pretrained(model_name, config=model_config)\n",
    "model = AutoModelForMaskedLM.from_pretrained('klue-roberta-pretrained3')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.parameters\n",
    "model.to(device)\n",
    "\n",
    "data_collator = MyDataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aada4942-40d7-45b8-8624-084887ae8ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6897\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8630\n",
      "  Number of trainable parameters = 336690432\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjstep750\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/ml/input/level2_mrc_nlp-level2-nlp-05/wandb/run-20230104_150823-2m6yoe7w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jstep750/huggingface/runs/2m6yoe7w\" target=\"_blank\">./klue-roberta-pretrained_qa2</a></strong> to <a href=\"https://wandb.ai/jstep750/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8630' max='8630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8630/8630 39:07, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in ./klue-roberta-pretrained_qa2/config.json\n",
      "Model weights saved in ./klue-roberta-pretrained_qa2/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# cuda out-of-memory 발생하여 fp16 = True 로 변경\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./klue-roberta-pretrained_qa2\",\n",
    "    learning_rate=3e-05,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_steps=5000,\n",
    "    save_total_limit=1,\n",
    "    save_strategy=\"steps\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=200,\n",
    "    fp16=True, # 16비트로 변환\n",
    "    fp16_opt_level=\"O1\",\n",
    "    resume_from_checkpoint=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./klue-roberta-pretrained_qa2\")  # pretrained_model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c0cb74b-8f74-4df7-af75-6b7417b59296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan  4 04:45:12 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   38C    P0    36W / 250W |  28445MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e7d95-b35b-4462-ac03-a21ffdeef642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
