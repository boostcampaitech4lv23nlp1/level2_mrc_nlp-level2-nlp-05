{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350c16ce-8f43-4253-b78a-d599221ee1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63720\n",
      "63720\n",
      "127440\n",
      "{'document_text': '이 목록에 실린 국가 기준은 1933년 몬테비데오 협약 1장을 참고로 하였다. 협정에 따르면, 국가는 다음의 조건을 만족해야 한다.\\n* (a) 영속적인 국민\\n* (b) 일정한 영토\\n* (c) 정부\\n* (d) 타국과의 관계 참여 자격.\\n특히, 마지막 조건은 국제 공동체의 참여 용인을 내포하고 있기 때문에, 다른 나라의 승인이 매우 중요한 역할을 할 수 있다.  이 목록에 포함된 모든 국가는 보통 이 기준을 만족하는 것으로 보이는 자주적이고 독립적인 국가이다. 하지만 몬테비데오 협약 기준을 만족하는지의 여부는 많은 국가가 논쟁이 되고 있는 실정이다. 또한, 몬테비데오 협약 기준만이 국가 지위의 충분한 자격이든 아니든, 국제법의 견해 차이는 존재할 수 있다. 이 물음에 대한 다른 이론에 대한 고리는 아래에서 볼 수 있다.\\n\\n위 기준에 논거하여 이 목록은 다음 206개 국가를 포함하고 있다.\\n* 일반 국제 승인을 받은 195개 자주 국가.\\n** 유엔 가입 국가 193개\\n** 성좌의 명칭으로', 'masked_strings': '이 목록에 실린 국가 기준은 [MASK][MASK] [MASK][MASK][MASK][MASK] [MASK][MASK]을 참고로 하였다. 협정에 따르면, 국가는 다음의 조건을 만족해야 한다. * ([MASK]) 영속적인 [MASK] * (b) 일정한 영토 * (c) [MASK] * (d) 타국과의 관계 참여 자격. 특히, 마지막 조건은 국제 공동체의 참여 용인을 내포하고 있기 때문에, 다른 나라의 승인이 매우 중요한 역할을 할 수 있다. 이 목록에 포함된 모든 국가는 보통 이 기준을 만족하는 것으로 보이는 자주적이고 독립적인 국가이다. 하지만 [MASK][MASK][MASK]오 협약 기준을 만족하는지의 여부는 많은 국가가 논쟁이 되고 있는 실정이다. 또한, [MASK][MASK][MASK] 협약 기준만이 국가 지위의 충분한 자격이든 아니든, [MASK]의 견해 차이는 존재할 수 있다. 이 물음에 대한 다른 이론에 대한 고리는 [MASK]에서 볼 수 있다. 위 기준에 논거하여 이 목록은 다음 206개 국가를 포함하고 있다. * 일반 국제 승인을 받은 [MASK][MASK] 자주 국가. ** 유엔 가입 국가 193개 ** 성좌의 명칭으로', 'inputs': [1504, 10188, 2170, 11381, 3728, 3872, 2073, 4, 4, 4, 4, 4, 4, 4, 4, 1498, 6108, 2200, 1889, 2507, 2062, 18, 6323, 2170, 3881, 2460, 16, 3728, 2259, 3729, 2079, 4423, 2069, 4671, 8084, 3605, 18, 14, 12, 4, 13, 27515, 31221, 4, 14, 12, 69, 13, 4362, 2470, 8683, 14, 12, 70, 13, 4, 14, 12, 71, 13, 24480, 2145, 2079, 3654, 3844, 5109, 18, 3727, 16, 4178, 4423, 2073, 3854, 5516, 2079, 3844, 6623, 2069, 11160, 19521, 1513, 2015, 3624, 2170, 16, 3656, 3779, 2079, 5887, 2052, 4230, 3748, 2470, 4008, 2069, 1892, 1295, 1513, 2062, 18, 1504, 10188, 2170, 3954, 2897, 3721, 3728, 2259, 4865, 1504, 3872, 2069, 4671, 2205, 2259, 575, 6233, 3783, 2259, 4695, 2125, 2052, 2088, 4751, 31221, 3728, 28674, 18, 3696, 4, 4, 4, 1443, 5598, 3872, 2069, 4671, 2205, 18246, 2079, 4588, 2259, 1039, 2073, 3728, 2116, 6851, 2052, 859, 2088, 1513, 2259, 7123, 28674, 18, 3819, 16, 4, 4, 4, 5598, 3872, 2154, 2052, 3728, 6401, 2079, 4332, 2470, 5109, 2052, 2778, 3614, 2778, 16, 4, 1503, 6270, 4193, 2259, 3990, 2085, 1295, 1513, 2062, 18, 1504, 9164, 2170, 3618, 3656, 4748, 2170, 3618, 7403, 2259, 4, 3604, 1164, 1295, 1513, 2062, 18, 1485, 3872, 2170, 25307, 7488, 1504, 10188, 2073, 3729, 21732, 2019, 3728, 2138, 3954, 19521, 1513, 2062, 18, 14, 3935, 3854, 5887, 2069, 1122, 2073, 4, 4, 4695, 3728, 18, 14, 14, 6125, 4502, 3728, 6026, 2019, 14, 14, 1268, 2584, 2079, 7864, 6233], 'labels': [1504, 10188, 2170, 11381, 3728, 3872, 2073, 20998, 2440, 25218, 2151, 28330, 5598, 21, 2121, 1498, 6108, 2200, 1889, 2507, 2062, 18, 6323, 2170, 3881, 2460, 16, 3728, 2259, 3729, 2079, 4423, 2069, 4671, 8084, 3605, 18, 14, 12, 68, 13, 27515, 31221, 3735, 14, 12, 69, 13, 4362, 2470, 8683, 14, 12, 70, 13, 3659, 14, 12, 71, 13, 24480, 2145, 2079, 3654, 3844, 5109, 18, 3727, 16, 4178, 4423, 2073, 3854, 5516, 2079, 3844, 6623, 2069, 11160, 19521, 1513, 2015, 3624, 2170, 16, 3656, 3779, 2079, 5887, 2052, 4230, 3748, 2470, 4008, 2069, 1892, 1295, 1513, 2062, 18, 1504, 10188, 2170, 3954, 2897, 3721, 3728, 2259, 4865, 1504, 3872, 2069, 4671, 2205, 2259, 575, 6233, 3783, 2259, 4695, 2125, 2052, 2088, 4751, 31221, 3728, 28674, 18, 3696, 25218, 2151, 2147, 1443, 5598, 3872, 2069, 4671, 2205, 18246, 2079, 4588, 2259, 1039, 2073, 3728, 2116, 6851, 2052, 859, 2088, 1513, 2259, 7123, 28674, 18, 3819, 16, 25218, 2151, 28330, 5598, 3872, 2154, 2052, 3728, 6401, 2079, 4332, 2470, 5109, 2052, 2778, 3614, 2778, 16, 20747, 1503, 6270, 4193, 2259, 3990, 2085, 1295, 1513, 2062, 18, 1504, 9164, 2170, 3618, 3656, 4748, 2170, 3618, 7403, 2259, 4402, 3604, 1164, 1295, 1513, 2062, 18, 1485, 3872, 2170, 25307, 7488, 1504, 10188, 2073, 3729, 21732, 2019, 3728, 2138, 3954, 19521, 1513, 2062, 18, 14, 3935, 3854, 5887, 2069, 1122, 2073, 6012, 2019, 4695, 3728, 18, 14, 14, 6125, 4502, 3728, 6026, 2019, 14, 14, 1268, 2584, 2079, 7864, 6233]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./dataset/wikipedia_documents_ssm.json\", \"r\") as f1, open(\"./dataset/wikipedia_documents_ssm1.json\", \"r\") as f2:\n",
    "    wiki1 = json.load(f1)\n",
    "    print(len(wiki1))\n",
    "    wiki2 = json.load(f2)\n",
    "    print(len(wiki2))\n",
    "    wiki3 = wiki1 + wiki2\n",
    "    print(len(wiki3))\n",
    "    print(wiki3[0])\n",
    "    with open(\"./dataset/wikipedia_documents_ssm2.json\", \"w\") as f:\n",
    "        json.dump(wiki3, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6473efd-b523-48c5-9150-adab3fcf16ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "import argparse\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "from collections.abc import Mapping\n",
    "from dataclasses import dataclass\n",
    "from random import randint\n",
    "from collections import defaultdict\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "import torch\n",
    "\n",
    "class MyDataCollatorWithPadding(DataCollatorWithPadding):\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        max_len = 0\n",
    "        for i in features:\n",
    "            if len(i[\"input_ids\"]) > max_len:\n",
    "                max_len = len(i[\"input_ids\"])\n",
    "        \n",
    "        batch = defaultdict(list)\n",
    "        for item in features:\n",
    "            for k in item:\n",
    "                item[k] = torch.Tensor(item[k])\n",
    "                padding_len = max_len - item[k].size(0)\n",
    "                if k == \"input_ids\":\n",
    "                    item[k] = torch.cat((item[k], torch.tensor([self.tokenizer.pad_token_id] * padding_len)), dim=0)\n",
    "                else:\n",
    "                    item[k] = torch.cat((item[k], torch.tensor([0] * padding_len)), dim=0)\n",
    "                batch[k].append(item[k])\n",
    "\n",
    "        for k in batch:\n",
    "            batch[k] = torch.stack(batch[k], dim=0)\n",
    "            batch[k] = batch[k].to(torch.long)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "    \n",
    "def load_eval_dataset(tokenizer, datasets):\n",
    "    # preprocessing\n",
    "    def prepare_validation_features(examples):\n",
    "        # truncation과 padding(length가 짧을때만)을 통해 toknization을 진행하며, stride를 이용하여 overflow를 유지합니다.\n",
    "        # 각 example들은 이전의 context와 조금씩 겹치게됩니다.\n",
    "        \n",
    "        final_examples = {}\n",
    "        final_examples['input_ids'] = examples['inputs']\n",
    "        final_examples['labels'] = examples['labels']\n",
    "        \n",
    "        final_examples['attention_mask'] = []\n",
    "        for i in examples['inputs']:\n",
    "            final_examples['attention_mask'].append([1]*len(i))\n",
    "        \n",
    "        return final_examples\n",
    "\n",
    "    train_dataset = datasets[\"train\"]\n",
    "\n",
    "    # Validation Feature 생성\n",
    "    train_dataset = train_dataset.map(\n",
    "        prepare_validation_features,\n",
    "        batched = True,\n",
    "        remove_columns=['document_text', 'masked_strings', 'inputs', 'labels']\n",
    "    )\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "def ssm_pretrain():\n",
    "    model_name = 'klue/roberta-large'\n",
    "\n",
    "    # set up tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    \n",
    "    # label 없이 가져오기 위해서 load_predict_dataset 사용\n",
    "    ### Refactoring 필요! ###\n",
    "    dataset = load_dataset(\"json\", data_files=\"./dataset/wikipedia_documents_ssm2.json\")\n",
    "    train_dataset = load_eval_dataset(tokenizer, dataset)\n",
    "\n",
    "    # Pretrained model for MaskedLM training\n",
    "    model_config = AutoConfig.from_pretrained(model_name)  # 모델 가중치 불러오기\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name, config=model_config)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "\n",
    "    data_collator = MyDataCollatorWithPadding(tokenizer)\n",
    "\n",
    "    # cuda out-of-memory 발생하여 fp16 = True 로 변경\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./klue-roberta-pretrained\",\n",
    "        learning_rate=3e-05,\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        save_steps=4000,\n",
    "        save_total_limit=2,\n",
    "        save_strategy=\"steps\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=100,\n",
    "        fp16=True, # 16비트로 변환\n",
    "        fp16_opt_level=\"O1\",\n",
    "        resume_from_checkpoint=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(\"./klue-roberta-pretrained\")  # pretrained_model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8717d1b0-5c31-4cad-850b-e9cf75828992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = [1,1,1,1,1]\n",
    "torch.Tensor(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bfec974-87fe-4b0d-b9a1-f2ddd01bc271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4123cfdacdd623e5\n",
      "Found cached dataset json (/opt/ml/.cache/huggingface/datasets/json/default-4123cfdacdd623e5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "100%|██████████| 1/1 [00:00<00:00, 520.39it/s]\n",
      "Loading cached processed dataset at /opt/ml/.cache/huggingface/datasets/json/default-4123cfdacdd623e5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-d31b2cc0ae2b3444.arrow\n"
     ]
    }
   ],
   "source": [
    "model_name = 'klue/roberta-large'\n",
    "\n",
    "# set up tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# label 없이 가져오기 위해서 load_predict_dataset 사용\n",
    "### Refactoring 필요! ###\n",
    "dataset = load_dataset(\"json\", data_files=\"../dataset/wikipedia_documents_ssm_qa2.json\")\n",
    "train_dataset = load_eval_dataset(tokenizer, dataset)\n",
    "\n",
    "# Pretrained model for MaskedLM training\n",
    "model_config = AutoConfig.from_pretrained(model_name)  # 모델 가중치 불러오기\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name, config=model_config)\n",
    "#model = AutoModelForMaskedLM.from_pretrained('klue-roberta-pretrained3')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.parameters\n",
    "model.to(device)\n",
    "\n",
    "data_collator = MyDataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aada4942-40d7-45b8-8624-084887ae8ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6897\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8630\n",
      "  Number of trainable parameters = 336690432\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjstep750\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/ml/input/level2_mrc_nlp-level2-nlp-05/ssm/wandb/run-20230105_083702-e5wi40zi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jstep750/huggingface/runs/e5wi40zi\" target=\"_blank\">./klue-roberta-pretrained_qa2</a></strong> to <a href=\"https://wandb.ai/jstep750/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='8630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   9/8630 00:01 < 39:28, 3.64 it/s, Epoch 0.01/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m      2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      3\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./klue-roberta-pretrained_qa2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-05\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     resume_from_checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     20\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     21\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     22\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./klue-roberta-pretrained_qa2\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# pretrained_model save\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1527\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1524\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1526\u001b[0m )\n\u001b[0;32m-> 1527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1837\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_grad_scaling:\n\u001b[1;32m   1836\u001b[0m     scale_before \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mget_scale()\n\u001b[0;32m-> 1837\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1838\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m   1839\u001b[0m     scale_after \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mget_scale()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:294\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 294\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:67\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     66\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:364\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    361\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    362\u001b[0m denom \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 364\u001b[0m step_size \u001b[38;5;241m=\u001b[39m \u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# No bias correction for Bert\u001b[39;00m\n\u001b[1;32m    366\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# cuda out-of-memory 발생하여 fp16 = True 로 변경\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./klue-roberta-pretrained_qa2\",\n",
    "    learning_rate=3e-05,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_steps=5000,\n",
    "    save_total_limit=1,\n",
    "    save_strategy=\"steps\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=200,\n",
    "    fp16=True, # 16비트로 변환\n",
    "    fp16_opt_level=\"O1\",\n",
    "    resume_from_checkpoint=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./klue-roberta-pretrained_qa2\")  # pretrained_model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c0cb74b-8f74-4df7-af75-6b7417b59296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan  4 04:45:12 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   38C    P0    36W / 250W |  28445MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e7d95-b35b-4462-ac03-a21ffdeef642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
