{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3130152e-4c79-4cb1-bd2e-f792b4e69eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 08:14:33 | INFO | pororo.models.brainbert.tasks.sequence_tagging | [input] dictionary: 4005 types\n",
      "2023-01-05 08:14:33 | INFO | pororo.models.brainbert.tasks.sequence_tagging | [label] dictionary: 41 types\n"
     ]
    }
   ],
   "source": [
    "from pororo import Pororo\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    ")\n",
    "\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "mu, sigma = 0, 0.1\n",
    "ner = Pororo(task=\"ner\", lang=\"ko\")\n",
    "\n",
    "model_name = 'klue/roberta-large'\n",
    "\n",
    "# set up tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0518f429-8be8-417e-a201-f11b81d4a333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3952 3952\n",
      "4192 4192\n"
     ]
    }
   ],
   "source": [
    "data = load_from_disk('../dataset/train_dataset')\n",
    "data\n",
    "\n",
    "contexts = []\n",
    "answers = []\n",
    "for d in data['train']:\n",
    "    contexts.append(d['context'])\n",
    "    answers.append(d['answers']['text'][0])\n",
    "    \n",
    "print(len(contexts), len(answers))\n",
    "\n",
    "for d in data['validation']:\n",
    "    contexts.append(d['context'])\n",
    "    answers.append(d['answers']['text'][0])\n",
    "    \n",
    "print(len(contexts), len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92dce52d-fbe9-47f0-897e-1d839ca24a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 동아대학교박물관에서 소장하고 있는 4 계사명 5개 사리구는 총 4개의 용기로 구성된 조선후기의 유물로 ,\n",
      "[5165, 2104, 4622, 2042, 2266, 2133, 27135, 5366, 19521, 1513, 2259, 24, 593, 2063, 2211, 25, 2019, 11955, 2251, 2259, 1668, 24, 2019, 2079, 6153, 2200, 3896, 2897, 3957, 2158, 2015, 2079, 9291, 2200, 16]\n",
      "[5165, 2104, 4622, 2042, 2266, 2133, 27135, 5366, 19521, 1513, 2259, 24, 593, 2063, 2211, 4, 4, 11955, 2251, 2259, 1668, 4, 4, 2079, 6153, 2200, 3896, 2897, 3957, 2158, 2015, 2079, 9291, 2200, 16]\n",
      "동아대학교박물관에서 소장하고 있는 4 계사명 [MASK] [MASK] 사리구는 총 [MASK] [MASK]의 용기로 구성된 조선후기의 유물로,\n",
      "동아대학교박물관에서 소장하고 있는 4 계사명 5개 사리구는 총 4개의 용기로 구성된 조선후기의 유물로,\n",
      "35 35\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "i = 100\n",
    "context = '동아대학교박물관에서 소장하고 있는 4 계사명 5개 사리구는 총 4개의 용기로 구성된 조선후기의 유물로,'\n",
    "answer = '4개'\n",
    "\n",
    "string = ''\n",
    "labels = []\n",
    "inputs = []\n",
    "tokenized_text = tokenizer.tokenize(context)\n",
    "tokenized_ids = tokenizer(context)['input_ids'][1:-1]\n",
    "tokenized_ans = tokenizer.tokenize(answer)\n",
    "n = 0\n",
    "for text, ids in zip(tokenized_text, tokenized_ids):\n",
    "    labels.append(ids)\n",
    "    inputs.append(ids)\n",
    "    add = text[2:] if '##' in text else ' '+text\n",
    "    string += add\n",
    "    \n",
    "    if(text == tokenized_ans[n]):\n",
    "        n += 1\n",
    "        end = len(tokenized_ans)\n",
    "        if(n == end):\n",
    "            inputs = inputs[:-end] + tokenizer('[MASK]' * len(tokenized_ans))['input_ids'][1:-1]\n",
    "            n = 0\n",
    "    #else: n = 0\n",
    "        \n",
    "print(string)\n",
    "print(labels)\n",
    "print(inputs)\n",
    "print(tokenizer.decode(inputs))\n",
    "print(tokenizer.decode(labels))\n",
    "print(len(labels), len(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cded9916-f7d2-4eb5-9e2a-50167d7ae4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['동아',\n",
       " '##대',\n",
       " '##학교',\n",
       " '##박',\n",
       " '##물',\n",
       " '##관',\n",
       " '##에서',\n",
       " '소장',\n",
       " '##하고',\n",
       " '있',\n",
       " '##는',\n",
       " '4',\n",
       " '계',\n",
       " '##사',\n",
       " '[MASK]',\n",
       " '[MASK]',\n",
       " '사리',\n",
       " '##구',\n",
       " '##는',\n",
       " '총',\n",
       " '[MASK]',\n",
       " '[MASK]',\n",
       " '의',\n",
       " '용기',\n",
       " '##로',\n",
       " '구성',\n",
       " '##된',\n",
       " '조선',\n",
       " '##후',\n",
       " '##기',\n",
       " '##의',\n",
       " '유물',\n",
       " '##로',\n",
       " ',']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(tokenizer.decode(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3f16d316-a724-4000-b8d7-df000f49c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_mask():\n",
    "    data = load_from_disk('../dataset/train_dataset')\n",
    "    \n",
    "    contexts = []\n",
    "    answers = []\n",
    "    for d in data['train']:\n",
    "        contexts.append(d['context'])\n",
    "        answers.append(d['answers']['text'][0])\n",
    "        \n",
    "    for d in data['validation']:\n",
    "        contexts.append(d['context'])\n",
    "        answers.append(d['answers']['text'][0])\n",
    "    \n",
    "    wiki_contexts = []\n",
    "    wiki_answers = []\n",
    "    for t, a in zip(contexts, answers):\n",
    "        while(len(t) > 500):\n",
    "            wiki_contexts.append(t[:500])\n",
    "            wiki_answers.append(a)\n",
    "            t = t[384:]  # stride = 500 - 384 \n",
    "    \n",
    "    corpus = []\n",
    "    for context, answer in zip(tqdm(wiki_contexts), wiki_answers):\n",
    "        labels = []\n",
    "        inputs = []\n",
    "        tokenized_text = tokenizer.tokenize(context)\n",
    "        tokenized_ids = tokenizer(context)['input_ids'][1:-1]\n",
    "        tokenized_ans = tokenizer.tokenize(answer)\n",
    "        \n",
    "        n = 0\n",
    "        for text, ids in zip(tokenized_text, tokenized_ids):\n",
    "            labels.append(ids)\n",
    "            inputs.append(ids)\n",
    "            \n",
    "            if(text == tokenized_ans[n]):\n",
    "                n += 1\n",
    "                end = len(tokenized_ans)\n",
    "                if(n == end):\n",
    "                    inputs = inputs[:-end] + tokenizer('[MASK]' * len(tokenized_ans))['input_ids'][1:-1]\n",
    "                    n = 0\n",
    "            else: n = 0\n",
    "        \n",
    "        corpus.append({\"document_text\": tokenizer.decode(labels), \"masked_strings\": tokenizer.decode(inputs), \"inputs\":inputs, \"labels\":labels})\n",
    "        \n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "676dee94-54cf-420d-bc57-2a86fa9ad757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6897/6897 [00:23<00:00, 298.25it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = load_and_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f8ce76b8-ad0e-4b04-b242-1dfda4b2f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def make_data():\n",
    "    with open(\"./dataset/wikipedia_documents_ssm_qa2.json\", \"w\") as f:\n",
    "        json.dump(load_and_mask(), f)\n",
    "        #json.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fc5482fb-aa4e-4b4f-b87f-b880a6c906f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6897/6897 [00:23<00:00, 298.55it/s]\n"
     ]
    }
   ],
   "source": [
    "make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2745751e-29d5-479a-9a73-58dc49000bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__', 'post', 'embedding'],\n",
       "        num_rows: 3952\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__', 'post', 'embedding'],\n",
       "        num_rows: 240\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "a = load_from_disk('./embedding_train')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a9fbfac-069f-4e58-a3af-6cdc93ff07cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_dataset_emb(datasets, max_seq_length, tokenizer):\n",
    "    print(datasets[\"train\"])\n",
    "    # dataset을 전처리합니다.\n",
    "    # training과 evaluation에서 사용되는 전처리는 아주 조금 다른 형태를 가집니다.\n",
    "    column_names = datasets[\"train\"].column_names\n",
    "\n",
    "    question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
    "    context_column_name = \"post\" if \"post\" in column_names else column_names[7]\n",
    "    answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n",
    "    # Padding에 대한 옵션을 설정합니다.\n",
    "    # (question|context) 혹은 (context|question)로 세팅 가능합니다.\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "    # Train preprocessing / 전처리를 진행합니다.\n",
    "    def prepare_train_features(examples):  # example에는 dataset에 있는 item이 하나씩 들어갑니다. \n",
    "        # examples: [question, context, answers, title, id, document_id] -> 이 중 question, context, answers만 사용\n",
    "        # truncation과 padding(length가 짧을때만)을 통해 toknization을 진행하며, stride를 이용하여 overflow를 유지합니다.\n",
    "        # 각 example들은 이전의 context와 조금씩 겹치게됩니다.\n",
    "        \n",
    "        tokenized_examples = tokenizer(\n",
    "            examples[question_column_name if pad_on_right else context_column_name], # 첫번째 문장(question)\n",
    "            examples[context_column_name if pad_on_right else question_column_name], # 두번째 문장(context)\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=max_seq_length,\n",
    "            stride=128,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            return_token_type_ids=False if 'roberta' in tokenizer.name_or_path else True, # roberta모델을 사용할 경우 False, bert를 사용할 경우 True로 표기해야합니다.\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        \n",
    "        tokenized_examples_emb = tokenizer(\n",
    "            examples[question_column_name if pad_on_right else context_column_name],\n",
    "            examples['embedding'],\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=max_seq_length,\n",
    "            stride=128,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            return_token_type_ids=False if 'roberta' in tokenizer.name_or_path else True, # roberta모델을 사용할 경우 False, bert를 사용할 경우 True로 표기해야합니다.\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        \n",
    "        for i,v in enumerate(tokenized_examples_emb[\"input_ids\"]):\n",
    "            tokenized_examples_emb[\"input_ids\"][i] = [n if n==1 else 0 for n in v] \n",
    "        \n",
    "        # 길이가 긴 context가 등장할 경우 truncate를 진행해야하므로, 해당 데이터셋을 찾을 수 있도록 mapping 가능한 값이 필요합니다.\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        # token의 캐릭터 단위 position를 찾을 수 있도록 offset mapping을 사용합니다.\n",
    "        # start_positions과 end_positions을 찾는데 도움을 줄 수 있습니다.\n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "        # 데이터셋에 \"start position\", \"enc position\" label을 부여합니다.\n",
    "        tokenized_examples[\"start_positions\"] = []\n",
    "        tokenized_examples[\"end_positions\"] = []\n",
    "        tokenized_examples[\"inputs_embeds\"] = tokenized_examples_emb[\"input_ids\"]\n",
    "\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)  # cls index\n",
    "\n",
    "            # sequence id를 설정합니다 (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "            # 하나의 example이 여러개의 span을 가질 수 있습니다.\n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = examples[answer_column_name][sample_index]\n",
    "\n",
    "            # answer가 없을 경우 cls_index를 answer로 설정합니다(== example에서 정답이 없는 경우 존재할 수 있음).\n",
    "            if len(answers[\"answer_start\"]) == 0:\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # text에서 정답의 Start/end character index\n",
    "                start_char = answers[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "                # text에서 current span의 Start token index\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                    token_start_index += 1\n",
    "\n",
    "                # text에서 current span의 End token index\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                    token_end_index -= 1\n",
    "\n",
    "                # 정답이 span을 벗어났는지 확인합니다(정답이 없는 경우 CLS index로 label되어있음).\n",
    "                if not (\n",
    "                    offsets[token_start_index][0] <= start_char\n",
    "                    and offsets[token_end_index][1] >= end_char\n",
    "                ):\n",
    "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                else:\n",
    "                    # token_start_index 및 token_end_index를 answer의 끝으로 이동합니다.\n",
    "                    # Note: answer가 마지막 단어인 경우 last offset을 따라갈 수 있습니다(edge case).\n",
    "                    while (\n",
    "                        token_start_index < len(offsets)\n",
    "                        and offsets[token_start_index][0] <= start_char\n",
    "                    ):\n",
    "                        token_start_index += 1\n",
    "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "        return tokenized_examples\n",
    "\n",
    "    if \"train\" not in datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = datasets[\"train\"]\n",
    "\n",
    "    # dataset에서 train feature를 생성합니다.\n",
    "    train_dataset = train_dataset.map(\n",
    "        prepare_train_features,\n",
    "        batched=True,  # defalut batch size는 1000입니다.\n",
    "        remove_columns=column_names,  \n",
    "        # 기존 column_names인 ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__']를 삭제하고 ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']를 추가합니다.\n",
    "        \n",
    "    )\n",
    "    print(train_dataset)\n",
    "    return train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dcf5740-6f0d-44ac-8ee1-3f37ae8c5a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__', 'post', 'embedding'],\n",
      "    num_rows: 3952\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:06<00:00,  1.69s/ba]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions', 'inputs_embeds'],\n",
      "    num_rows: 5707\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "special_tokens_dict = {\"additional_special_tokens\": [\"[MASK]\", \"\\\\n\", \"\\\\\"]}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "datasets = a\n",
    "x = load_train_dataset_emb(datasets, 512, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "daf6104b-2aa0-42c8-96c9-31295e9698d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 512\n"
     ]
    }
   ],
   "source": [
    "print(len(x[2]['input_ids']), len(x[2]['inputs_embeds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bbace7-8661-4e58-981c-d0dfe1a68220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
